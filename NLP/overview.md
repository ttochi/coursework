# NLP study 개요

아래 내용에 대하여 입력, 출력, 기능, 입력이 흘러서 출력에 도달하는 과정을 자세히 그림 그리고 설명할 것

- L1 regularization
- L2 regularization
- Batch normalization
- Layer normalization
- Auto-encoders
- Recurrent Neural Networks
- Attention
- Seq2Seq
- Transformers
  - Decoders in transformers
  - Encoders in transformers
- Self-Attention
- Positional embeddings
- BERT
- GPT


# 딥 러닝을 이용한 자연어 처리 입문
https://wikidocs.net/book/2155

[기본과정]

1. Text preprocessing
  - Tokenization
  - Cleaning and Normalization

2. Conventional Language Model

3. Vector Similarity

4. 머신러닝 개요
  - Linear Regression
  - Logistic Regression
  - Softmax Regression

5. 딥러닝 개요
  - 퍼셉트론
  - Back propagation
  - Overfitting (Regularization)
  - Gradient vanishing, exploding
  - Multi-layer Perceptron

6. RNN
  - RNN (Recurrent Neural Network)
  - LSTM (Long Short-Term Memory)
  - GRU (Gate Recurrent Unit)

7. Word Embedding
  - Word2Vec
  - ELMo

8. CNN in NLP

> feed foward neural net
fully-connected layer
loss function에 대해서도 좀 더 알아봐야할 듯... 이게 어떻게 나오는 식이고 어떻게 계산되는가

[심화과정]

1. Seq2Seq
2. Attention
3. Transformer
4. BERT
5. GPT
